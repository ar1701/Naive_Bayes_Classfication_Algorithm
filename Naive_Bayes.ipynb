import pandas as pd          # For data manipulation and analysis
import numpy as np          # For numerical operations
from sklearn.model_selection import train_test_split  # For splitting data
from sklearn.preprocessing import StandardScaler      # For feature scaling
from sklearn.naive_bayes import GaussianNB           # The Naive Bayes classifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # For evaluation
import seaborn as sns       # For statistical data visualization
import matplotlib.pyplot as plt  # For creating plots
df = pd.read_csv('./sample_data/data.csv')
print("Dataset Shape:", df.shape)
print("\nMissing Values:\n", df.isnull().sum())
df
features_to_drop = ['id']
if 'Unnamed: 32' in df.columns:
    features_to_drop.append('Unnamed: 32')
df = df.drop(features_to_drop, axis=1)
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})
X = df.drop('diagnosis', axis=1)  # Features
y = df['diagnosis']               # Target variable
X_train_initial, X_test, y_train_initial, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
scaler = StandardScaler()
X_train_initial_scaled = scaler.fit_transform(X_train_initial)
X_test_scaled = scaler.transform(X_test)
print("\nClass Distribution in Training Set:")
print(y_train_initial.value_counts(normalize=True))
X_train, X_val, y_train, y_val = train_test_split(
    X_train_initial_scaled, y_train_initial,
    test_size=0.2, random_state=42, stratify=y_train_initial
)
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
y_val_pred = nb_model.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
print("\nValidation Accuracy:", round(val_accuracy * 100, 2), "%")
val_cm = confusion_matrix(y_val, y_val_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(val_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (Validation Set)')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()
print("\nClassification Report (Validation Set):")
print(classification_report(y_val, y_val_pred))
y_test_pred = nb_model.predict(X_test_scaled)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("\nTest Set Accuracy:", round(test_accuracy * 100, 2), "%")
# Display test set confusion matrix
test_cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (Test Set)')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()
print("\nClassification Report (Test Set):")
print(classification_report(y_test, y_test_pred))
# First, save the trained model
import joblib

# Save the model and scaler
joblib.dump(nb_model, 'naive_bayes_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

# Function to preprocess new data and make predictions
def predict_breast_cancer(new_data):
    """
    Make predictions on new unseen data.

    Parameters:
    new_data (pd.DataFrame): DataFrame containing new patient data

    Returns:
    dict: Dictionary containing prediction and probability
    """
    # Load the saved model and scaler
    loaded_model = joblib.load('naive_bayes_model.pkl')
    loaded_scaler = joblib.load('scaler.pkl')

    # Ensure the new data has the same features as training data
    required_features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
                        'smoothness_mean', 'compactness_mean', 'concavity_mean',
                        'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
                        'radius_se', 'texture_se', 'perimeter_se', 'area_se',
                        'smoothness_se', 'compactness_se', 'concavity_se',
                        'concave points_se', 'symmetry_se', 'fractal_dimension_se',
                        'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',
                        'smoothness_worst', 'compactness_worst', 'concavity_worst',
                        'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']

    # Check if all required features are present
    missing_features = set(required_features) - set(new_data.columns)
    if missing_features:
        raise ValueError(f"Missing features: {missing_features}")

    # Select and order features correctly
    new_data = new_data[required_features]

    # Scale the features
    scaled_data = loaded_scaler.transform(new_data)

    # Make prediction
    prediction = loaded_model.predict(scaled_data)
    prediction_proba = loaded_model.predict_proba(scaled_data)

    # Create results dictionary
    results = {
        'prediction': 'Malignant' if prediction[0] == 1 else 'Benign',
        'prediction_probability': {
            'Benign': round(prediction_proba[0][0] * 100, 2),
            'Malignant': round(prediction_proba[0][1] * 100, 2)
        }
    }

    return results

# Example usage with new data
# Here's how to use the function with new patient data:

# Example 1: Single patient data
new_patient_data = pd.DataFrame({
    'radius_mean': [17.99],
    'texture_mean': [10.38],
    'perimeter_mean': [122.8],
    'area_mean': [1001.0],
    'smoothness_mean': [0.1184],
    'compactness_mean': [0.2776],
    'concavity_mean': [0.3001],
    'concave points_mean': [0.1471],
    'symmetry_mean': [0.2419],
    'fractal_dimension_mean': [0.07871],
    'radius_se': [1.095],
    'texture_se': [0.9053],
    'perimeter_se': [8.589],
    'area_se': [153.4],
    'smoothness_se': [0.006399],
    'compactness_se': [0.04904],
    'concavity_se': [0.05373],
    'concave points_se': [0.01587],
    'symmetry_se': [0.03003],
    'fractal_dimension_se': [0.006193],
    'radius_worst': [25.38],
    'texture_worst': [17.33],
    'perimeter_worst': [184.6],
    'area_worst': [2019.0],
    'smoothness_worst': [0.1622],
    'compactness_worst': [0.6656],
    'concavity_worst': [0.7119],
    'concave points_worst': [0.2654],
    'symmetry_worst': [0.4601],
    'fractal_dimension_worst': [0.1189]
})

# Make prediction
result = predict_breast_cancer(new_patient_data)
print("\nPrediction Results:")
print(f"Diagnosis: {result['prediction']}")
print("\nProbability Distribution:")
print(f"Benign: {result['prediction_probability']['Benign']}%")
print(f"Malignant: {result['prediction_probability']['Malignant']}%")

# Example 2: Multiple patients (batch prediction)
def batch_predict(data_path):
    """
    Make predictions on multiple patients from a CSV file.

    Parameters:
    data_path (str): Path to CSV file containing patient data

    Returns:
    pd.DataFrame: DataFrame with original data and predictions
    """
    # Load the data
    batch_data = pd.read_csv(data_path)

    # Make predictions
    predictions = []
    probabilities_benign = []
    probabilities_malignant = []

    for idx, row in batch_data.iterrows():
        result = predict_breast_cancer(pd.DataFrame([row]))
        predictions.append(result['prediction'])
        probabilities_benign.append(result['prediction_probability']['Benign'])
        probabilities_malignant.append(result['prediction_probability']['Malignant'])

    # Add predictions to the original data
    batch_data['Predicted_Diagnosis'] = predictions
    batch_data['Probability_Benign'] = probabilities_benign
    batch_data['Probability_Malignant'] = probabilities_malignant

    return batch_data

# Example usage for batch prediction:
# results_df = batch_predict('path_to_new_patients.csv')
# print(results_df[['Predicted_Diagnosis', 'Probability_Benign', 'Probability_Malignant']])
